import { PrismaClient } from '@awcrm/database';\nimport { cache } from './cache';\nimport { performanceMonitor } from './performance';\n\ninterface QueryCacheOptions {\n  ttl?: number; // Time to live in seconds\n  key?: string; // Custom cache key\n  tags?: string[]; // Cache tags for invalidation\n}\n\ninterface PaginationOptions {\n  page?: number;\n  limit?: number;\n  cursor?: string;\n}\n\nclass DatabaseOptimizer {\n  private prisma: PrismaClient;\n  private readonly defaultCacheTTL = 300; // 5 minutes\n  private readonly maxPageSize = 100;\n\n  constructor(prisma: PrismaClient) {\n    this.prisma = prisma;\n  }\n\n  // Cached query execution\n  async cachedQuery<T>(\n    queryFn: () => Promise<T>,\n    cacheOptions: QueryCacheOptions = {}\n  ): Promise<T> {\n    const {\n      ttl = this.defaultCacheTTL,\n      key = this.generateCacheKey(queryFn.toString()),\n      tags = [],\n    } = cacheOptions;\n\n    // Try to get from cache first\n    const cached = await cache.get<T>(key);\n    if (cached !== null) {\n      await performanceMonitor.recordCacheHit(key);\n      return cached;\n    }\n\n    // Cache miss - execute query\n    await performanceMonitor.recordCacheMiss(key);\n    const start = Date.now();\n    \n    try {\n      const result = await queryFn();\n      const duration = Date.now() - start;\n      \n      // Record query performance\n      await performanceMonitor.recordQuery(\n        queryFn.toString().substring(0, 200),\n        duration,\n        true\n      );\n      \n      // Cache the result\n      await cache.set(key, result, ttl);\n      \n      // Store cache tags for invalidation\n      if (tags.length > 0) {\n        for (const tag of tags) {\n          await cache.sadd(`cache_tags:${tag}`, key);\n        }\n      }\n      \n      return result;\n    } catch (error) {\n      const duration = Date.now() - start;\n      await performanceMonitor.recordQuery(\n        queryFn.toString().substring(0, 200),\n        duration,\n        false,\n        error instanceof Error ? error.message : 'Unknown error'\n      );\n      throw error;\n    }\n  }\n\n  // Invalidate cache by tags\n  async invalidateCacheByTags(tags: string[]): Promise<void> {\n    for (const tag of tags) {\n      const keys = await cache.smembers(`cache_tags:${tag}`);\n      if (keys.length > 0) {\n        for (const key of keys) {\n          await cache.del(key);\n        }\n        await cache.del(`cache_tags:${tag}`);\n      }\n    }\n  }\n\n  // Optimized pagination\n  async paginatedQuery<T>(\n    queryFn: (skip: number, take: number) => Promise<T[]>,\n    countFn: () => Promise<number>,\n    options: PaginationOptions = {}\n  ): Promise<{\n    data: T[];\n    pagination: {\n      page: number;\n      limit: number;\n      total: number;\n      totalPages: number;\n      hasNext: boolean;\n      hasPrev: boolean;\n    };\n  }> {\n    const {\n      page = 1,\n      limit = 20,\n    } = options;\n\n    // Validate pagination parameters\n    const validatedLimit = Math.min(Math.max(1, limit), this.maxPageSize);\n    const validatedPage = Math.max(1, page);\n    const skip = (validatedPage - 1) * validatedLimit;\n\n    // Execute queries in parallel\n    const [data, total] = await Promise.all([\n      this.cachedQuery(\n        () => queryFn(skip, validatedLimit),\n        {\n          key: `paginated:${queryFn.toString()}:${validatedPage}:${validatedLimit}`,\n          ttl: 60, // Short TTL for paginated data\n        }\n      ),\n      this.cachedQuery(\n        countFn,\n        {\n          key: `count:${countFn.toString()}`,\n          ttl: 300, // Longer TTL for counts\n        }\n      ),\n    ]);\n\n    const totalPages = Math.ceil(total / validatedLimit);\n\n    return {\n      data,\n      pagination: {\n        page: validatedPage,\n        limit: validatedLimit,\n        total,\n        totalPages,\n        hasNext: validatedPage < totalPages,\n        hasPrev: validatedPage > 1,\n      },\n    };\n  }\n\n  // Cursor-based pagination (more efficient for large datasets)\n  async cursorPaginatedQuery<T extends { id: string }>(\n    queryFn: (cursor?: string, take?: number) => Promise<T[]>,\n    options: PaginationOptions = {}\n  ): Promise<{\n    data: T[];\n    nextCursor?: string;\n    hasMore: boolean;\n  }> {\n    const {\n      cursor,\n      limit = 20,\n    } = options;\n\n    const validatedLimit = Math.min(Math.max(1, limit), this.maxPageSize);\n    \n    // Fetch one extra item to check if there are more\n    const data = await this.cachedQuery(\n      () => queryFn(cursor, validatedLimit + 1),\n      {\n        key: `cursor_paginated:${queryFn.toString()}:${cursor}:${validatedLimit}`,\n        ttl: 60,\n      }\n    );\n\n    const hasMore = data.length > validatedLimit;\n    const items = hasMore ? data.slice(0, validatedLimit) : data;\n    const nextCursor = hasMore && items.length > 0 ? items[items.length - 1].id : undefined;\n\n    return {\n      data: items,\n      nextCursor,\n      hasMore,\n    };\n  }\n\n  // Batch operations for better performance\n  async batchCreate<T>(\n    model: any,\n    data: any[],\n    batchSize: number = 100\n  ): Promise<T[]> {\n    const results: T[] = [];\n    \n    for (let i = 0; i < data.length; i += batchSize) {\n      const batch = data.slice(i, i + batchSize);\n      const batchResults = await this.prisma.$transaction(\n        batch.map(item => model.create({ data: item }))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  async batchUpdate<T>(\n    model: any,\n    updates: Array<{ where: any; data: any }>,\n    batchSize: number = 100\n  ): Promise<T[]> {\n    const results: T[] = [];\n    \n    for (let i = 0; i < updates.length; i += batchSize) {\n      const batch = updates.slice(i, i + batchSize);\n      const batchResults = await this.prisma.$transaction(\n        batch.map(update => model.update(update))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n\n  // Optimized search with full-text search capabilities\n  async searchQuery<T>(\n    queryFn: (searchTerm: string) => Promise<T[]>,\n    searchTerm: string,\n    options: {\n      minLength?: number;\n      cacheKey?: string;\n      ttl?: number;\n    } = {}\n  ): Promise<T[]> {\n    const {\n      minLength = 3,\n      cacheKey = `search:${queryFn.toString()}:${searchTerm}`,\n      ttl = 300,\n    } = options;\n\n    // Validate search term\n    if (searchTerm.length < minLength) {\n      return [];\n    }\n\n    // Sanitize search term\n    const sanitizedTerm = searchTerm.trim().toLowerCase();\n    \n    return this.cachedQuery(\n      () => queryFn(sanitizedTerm),\n      { key: cacheKey, ttl }\n    );\n  }\n\n  // Database health monitoring\n  async getDatabaseHealth(): Promise<{\n    status: 'healthy' | 'degraded' | 'unhealthy';\n    connectionCount: number;\n    avgQueryTime: number;\n    slowQueries: number;\n    errors: number;\n  }> {\n    try {\n      // Test basic connectivity\n      const start = Date.now();\n      await this.prisma.$queryRaw`SELECT 1`;\n      const pingTime = Date.now() - start;\n\n      // Get performance stats\n      const stats = await performanceMonitor.getPerformanceStats();\n      \n      let status: 'healthy' | 'degraded' | 'unhealthy' = 'healthy';\n      \n      if (stats.database.avgQueryTime > 1000 || stats.database.slowQueryCount > 10) {\n        status = 'degraded';\n      }\n      \n      if (pingTime > 5000 || stats.api.errorRate > 20) {\n        status = 'unhealthy';\n      }\n\n      return {\n        status,\n        connectionCount: 1, // Prisma manages connection pooling internally\n        avgQueryTime: stats.database.avgQueryTime,\n        slowQueries: stats.database.slowQueryCount,\n        errors: Math.round((stats.api.errorRate / 100) * stats.api.requestCount),\n      };\n    } catch (error) {\n      console.error('Database health check failed:', error);\n      return {\n        status: 'unhealthy',\n        connectionCount: 0,\n        avgQueryTime: 0,\n        slowQueries: 0,\n        errors: 1,\n      };\n    }\n  }\n\n  // Query optimization suggestions\n  async analyzeQueryPerformance(): Promise<{\n    suggestions: Array<{\n      type: 'index' | 'query' | 'cache';\n      message: string;\n      priority: 'low' | 'medium' | 'high';\n    }>;\n  }> {\n    const suggestions: Array<{\n      type: 'index' | 'query' | 'cache';\n      message: string;\n      priority: 'low' | 'medium' | 'high';\n    }> = [];\n\n    const queryMetrics = await performanceMonitor.getQueryMetrics(100);\n    const slowQueries = queryMetrics.filter(q => q.duration > 1000);\n    const failedQueries = queryMetrics.filter(q => !q.success);\n    \n    // Analyze slow queries\n    if (slowQueries.length > 5) {\n      suggestions.push({\n        type: 'index',\n        message: `${slowQueries.length} slow queries detected. Consider adding database indexes.`,\n        priority: 'high',\n      });\n    }\n\n    // Analyze failed queries\n    if (failedQueries.length > 0) {\n      suggestions.push({\n        type: 'query',\n        message: `${failedQueries.length} failed queries detected. Review query logic.`,\n        priority: 'high',\n      });\n    }\n\n    // Cache hit rate analysis\n    const stats = await performanceMonitor.getPerformanceStats();\n    if (stats.cache.hitRate < 70) {\n      suggestions.push({\n        type: 'cache',\n        message: `Cache hit rate is ${stats.cache.hitRate}%. Consider caching more queries.`,\n        priority: 'medium',\n      });\n    }\n\n    // Memory usage analysis\n    if (stats.memory.heapUsed > 500) {\n      suggestions.push({\n        type: 'query',\n        message: `High memory usage (${stats.memory.heapUsed}MB). Consider optimizing large queries.`,\n        priority: 'medium',\n      });\n    }\n\n    return { suggestions };\n  }\n\n  // Utility methods\n  private generateCacheKey(query: string): string {\n    // Create a hash of the query for consistent cache keys\n    const crypto = require('crypto');\n    return `query:${crypto.createHash('md5').update(query).digest('hex')}`;\n  }\n\n  // Transaction wrapper with performance monitoring\n  async transaction<T>(fn: (tx: any) => Promise<T>): Promise<T> {\n    const start = Date.now();\n    \n    try {\n      const result = await this.prisma.$transaction(fn);\n      const duration = Date.now() - start;\n      \n      await performanceMonitor.recordMetric('db.transaction.duration', duration);\n      \n      return result;\n    } catch (error) {\n      const duration = Date.now() - start;\n      await performanceMonitor.recordMetric('db.transaction.error', duration);\n      throw error;\n    }\n  }\n\n  // Bulk operations with optimized batching\n  async bulkUpsert<T>(\n    model: any,\n    data: Array<{ where: any; create: any; update: any }>,\n    batchSize: number = 50\n  ): Promise<T[]> {\n    const results: T[] = [];\n    \n    for (let i = 0; i < data.length; i += batchSize) {\n      const batch = data.slice(i, i + batchSize);\n      const batchResults = await this.prisma.$transaction(\n        batch.map(item => model.upsert(item))\n      );\n      results.push(...batchResults);\n    }\n    \n    return results;\n  }\n}\n\n// Create singleton instance\nconst dbOptimizer = new DatabaseOptimizer(new PrismaClient());\n\nexport { dbOptimizer };\nexport default dbOptimizer;\n"